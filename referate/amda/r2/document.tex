\documentclass[12pt]{article}

\usepackage{url} % websites in bib
\usepackage{hyperref}
\usepackage[a4paper,left=1in,right=1in,top=1in,bottom=1in]{geometry} % margins
\usepackage{paralist} % compact enumerates

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{amsmath}

\usepackage{multirow}

\usepackage{float} % figure placement

\begin{document}
	\title{Clustering beer text reviews using the K-means algorithm}
	\author{Stefan Sebastian, 242}
	\date{}
	\maketitle
	
	\begin{abstract}
	Beer is the most popular alcoholic drink and has been around for a long time. In addition, a craft beer revolution is going on at the moment with a lot of new varieties coming out and being actively discussed by enthusiasts. Beer marketers could profit if they had an automated way of processing the thousands of reviews published on beer dedicated websites. This paper presents such a solution of extracting information from beer review data by unsupervised learning. A dataset of around 85 thousand reviews is analyzed by extracting Tf-Idf information from each row and then passing that into a K-means clustering algorithm. The resulting clusters are then associated with the most common beer style of the elements inside. This allows us to measure the correlation between reviews and the type of beer being reviewed. A discussion is then made based on the terms with the largest weights for each cluster center. The model obtains a F1 score of 0.53 on a 9 label classification problem indicating that there is indeed a correlation between a beer's review and its style.
	\end{abstract}

	\newpage
	\tableofcontents
	\newpage
	
	\section{Introduction}
	\subsection{Motivation}
	Beer is one of the oldest and most popular alcoholic drinks. It is a staple drink in many societies and has been around ever since the formation of human civilizations, with evidence that dates it to more than 10,000 years ago\cite{BeerArticleTime}. It's popularity is attributed to having a moderate quantity of alcohol, which leads to more positive drinking experiences, and the fact that it can be drunk at any social event\cite{BeerArticleTime}.
	
	The global beer market was valued at 593\$ billion and is expected to rise to 685\$ billion by 2025\cite{BeerStats}. The most popular beer style is by far lager, but others are rising in popularity. Craft beer is a type of beer that is produced in microbreweries, in a traditional style and with an emphasis on taste and variety. The global craft beer market accounted for 38\$ billion and is growing at an explosive rate of 14.1\% each year\cite{CraftBeerStats}.
	
	It is obvious that there is a huge public interest in this drink, and any insights into what consumers think about beer could have serious economic benefits. This paper presents an experiment on extracting information using unsupervised learning on beer reviews. The main goals are finding natural clusters among beer descriptions and what words are used to describe similar beers. Also the clusters are analyzed by the most common beer styles to see if there is a correlation between reviews and beer types.
	
	\subsection{Paper outline}
	The introductory section contains the motivation for the experiment and some insights into the studied domain. A short list of objectives is also put forth. The second section is a short survey of related work in terms of text clustering, review analysis and beer data mining. Next, the dataset used for the experiment is presented along with the exploratory data analysis and preprocessing techniques.
	
	The solution description section starts with a detailed presentation of the proposed model for data analysis. This is followed by some implementation discussion in subsection 'Technologies used'. Finally, the reasoning behind the selection of parameters for the model is given.
	
	The evaluation section describes the metrics and methodologies used to measure the performance of the proposed model. A concrete set of results is presented which is followed by some discussion based on the shape and means of the discovered clusters. Finally, the conclusion reiterate the results and propose some plans for future research.
	
	\section{Related work}
	
	\subsection{Text clustering}
	A comprehensive review on text document clustering similarity measures was published by Anna Huang\cite{TextClustering}. The paper describes a complete text clustering process with emphasis on distance measurements. Features are extracted using the tf-idf method and the clustering algorithm is the basic K-means variant. A set of similarity measures are studies such as cosine similarity, euclidean distance, Jaccard coefficient, etc. The datasets considered vary from news to scientific articles.
	
	An implementation of a Scatter-Gather document browsing system was presented by Larsen and Aone\cite{ScatterGather}. The system applies document clustering on the whole dataset, then when the user selects a group, it clusters the selected group into smaller groups. This operation can be extends until each group contains only one document. The algorithm used by the system is K-means with an improvement in seed selection consisting in updating the initial means after every point assignment. The algorithm uses as a feature extraction method the tf-idf vectorizer.
	
	An interesting approach to text clustering is the one developed by Hu et al.\cite{WikipediaKmeans}. Their method consists of enriching text information by using Wikipedia concept and category data. A dataset was made from Wikipedia articles related to different concepts. When a document is evaluated the term weight is calculated using the combined tf-idf score from the training and the Wikipedia concept dataset. The category score is built by analyzing the most important concepts of the document. The clustering algorithm used in the experiment was spherical K-means. This method was tested on three datasets and was shown to obtain better results than simple clustering over tf-idf scores.
	
	\subsection{Review analysis}
	Most papers on review analysis are focused on extracting information about aspects present in the review. For example, Lu et al.\cite{LARA} propose an algorithm for inferring the aspects considered in a review and assigning a score to them, and use it on hotel reviews from tripadvisor. The algorithm, called LARA, works in two steps. In the first step, given some keywords for the aspects, it associates each sentence in a review with the aspect for which it has the biggest overlap. In the second step, it computes a score for each aspect based on the sentiments present in the sentences found in previous steps.
	
	Another direction of review analysis is to use linguistic rules in a constrained domain. For instance, Iacob and Rachel\cite{MARA}, created an algorithm for automatically extracting and scoring feature request from mobile application reviews. The algorithm uses a set of 237 rules which were build by manual analysis of review data. An example rule is "(adding) \textless request\textgreater would (\textless ADVERB\textgreater) be \textless POSITIVE-ADJECTIVE\textgreater". The Latent Dirichlet Allocation algorithm is then used on the discovered feature requests in order to find topics.
	
	\subsection{Experiments on beer datasets}
	McAuley and Leskovec\cite{ExperienceReview} built a recommender system that adapts to user experience. The motivation was that the users' preferences evolve as they acquire experience. The datasets used for the experiment included the BeerAdvocate review dataset\cite{BeerAdvocateData} because of the fact that beer is one of the most widespread acquired tastes. The experience label is learned by the model by using rating and timestamp data.
	
	Braun and Timpe\cite{BeerWineReviews} scrape data from BeerAdvocate, RateBeer and CellarTracker websites. These contain user text reviews and numerical ratings for different types of beer and wine. The goal of the experiment is to predict scores based on the given review. The features are extracted using the bag-of-words model for single words an bigrams. To obtain more relevant features, the input data is stemmed, cleared of stop words, and the tokens are tagged by part of speech and only adjectives, nouns, verbs and adverbs are kept. The algorithms used for classification are Naive Bayes and Support Vector Machines. The model obtained an accuracy of up to 87\%.
	
	\section{Dataset}
	The dataset used for this experiment is a collection of beer reviews taken from the BeerAdvocate website. The original dataset was made up out of 1.5 million user reviews, from 33387 different users, collected between 1998 and 2011. It is not available at this time but I found a subset of around 500 thousand reviews on data.world\cite{BeerAdvocateData}. 
	
	The data is in csv format, each row containing various information like: beer name, beer style, alcohol content, scores for taste, appearance, aroma and a textual review. The only columns considered for this experiment were the beer style and the text review.
	
	On exploratory data analysis, 407 duplicate and 119 missing reviews were found. Also there are 104 distinct beer styles and the amount of data for each of them is quite imbalanced as can be see in figure \ref{fig:initialDistribution}, where each bar represents a beer style with a size proportional to the number of reviews for it.
	
	\begin{figure}[h]
		\includegraphics[width=\linewidth]{resources/InitialDistribution.png}
		\caption{A plot of the number of reviews per beer style}
		\label{fig:initialDistribution}
	\end{figure}

	The first step in preprocessing this dataset was to remove rows containing duplicates and missing values in the columns that are relevant for the experiment: beer style, text review. Next, some of the styles for which there wasn't a lot of data available were dropped, as they would be outliers for clustering. The threshold for this cut was chosen arbitrarily at 7000 reviews. 
	
	Upon inspection of the beer styles present in the dataset, a fine granularity was observed. For example, the difference between American Pale Ale and American India Pale Ale is minimal and not very rigorous. This and other similar cases would only confuse the classifier. For this reason, a mapping was made for each style to a broader category, using the taxonomy published by BusinessInsider\cite{BeerTaxonomy}. 
	
	The final mapping can be seen in figure \ref{fig:styleMapping}. Finally, a balanced dataset is built by selecting the minimum value for which we have an equal distribution of reviews per beer style, which is around 9500 per style. The analyzed dataset is made through random sampling of 9500 values for each of the remaining beer styles.
	
	\begin{figure}
		\includegraphics[width=\linewidth]{resources/MappingDiagram.png}
		\caption{Mapping of beer styles from the original dataset}
		\label{fig:styleMapping}
	\end{figure}

	\section{Solution description}
	\subsection{Model overview}
	In order to process the textual data, some features need to be extracted by using natural language processing techniques. The method used to extract numerical features about the words in the dataset is the Tf-Idf vectorizer. Tf-Idf\cite{TfIdfBook} stands for term frequency-inverse document frequency and is the most popular method for weighting term. In brief, the importance of a word is proportional to the number of times it appears in a document and inversely proportional to the number of documents it appears in. Before applying Tf-Idf, the review texts are tokenized, the stop words are removed and then each word is stemmed. Stemming is the process of reducing a word to its root, or base form, and the algorithm selected for this is the Snowball Stemmer\cite{SnowballStemmer}, which works by defining a set of rules for replacing word endings. The number of terms considered as features has been determined experimentally. Finally, the dataset contains a list of array that denotes how important is each considered term for every review.
	
	To solve the clustering problem the classic K-Means algorithm is used with two different seed initialization methods: random values and k-d trees. The first method is very simple conceptually, just choose k random points from the dataset. The second method has been described by Redmon and Heneghan\cite{KdTreeKmeans}. The basic idea is to use this data structure to divide multidimensional features into buckets and the make density estimation depending on the size of the bucket and the volume of the bounding box.
	
	The resulting clusters are evaluated based on initial beer style labels and the dominant terms in each cluster.
	
	\begin{figure}[H]
		\includegraphics[width=\linewidth]{resources/model.png}
		\caption{Diagram of the transformations on the dataset}
		\label{fig:modelOverview}
	\end{figure}

	\subsection{Technologies used}
	The project was implemented in the Python programming language on the Anaconda distribution. The clustering and seed initialization algorithms were implemented from scratch.
	
	For data analysis and preprocessing the pandas\cite{Pandas} library was used, which provides some efficient data manipulation and analysis tools. The group by function was particularly useful for operations on all data of a particular beer style. 
	
	The algorithms for tokenizing and stemming were taken from the nltk library\cite{NLTK}, which is one of the most popular tools for natural language processing. The Tf-Idf vectorizer implementation was provided by the scikit-learn library\cite{sk-learn}, which contains a comprehensive collection of data analysis algorithms.
	
	In order to visualize the data and the results the following plotting tools were used. Matplotlib\cite{Matplotlib} provided a tool to make bar charts in order to observe the initial data distribution and Seaborn\cite{Seaborn} was used to generate a heatmap of the resulting clusters.
		
	\subsection{Parameters}
	The most important parameter used in the experiment is k, the number of clusters. This was chosen to be the number of distinct beer styles in the dataset, 9, as the reviews for beers of the same type should be similar to each other.
	
	The number of features for each data point is equivalent to how many word frequencies are considered. This was determined experimentally by running the basic algorithm with some different feature values. The results, shown in table \ref{tab:features}, indicate that until around 1000 there is the biggest growth in performance.
	
	\begin{center}
		\label{tab:features}
		\begin{tabular}{ |c|c|c|c| } 
			\hline
			Nr features & Precision & Recall & F1 \\
			\hline
			200 & 0.420 & 0.481 & 0.449 \\
			700 & 0.501 & 0.554 & 0.526 \\
			1000 & 0.537 & 0.532 & 0.534 \\
			1500 & 0.515 & 0.528 & 0.521 \\
			\hline
		\end{tabular}
	\end{center}

	Two methods were tested for seed initialization: random points and kd-tree based. The run with kd-trees obtained a 0.47 F1 score while, random initialization got a 0.53 F1 score. This might be caused by finding good starting points through randomness or that the large number of features is not a good fit for the kd-tree data structure.

	\section{Evaluation}
	\subsection{Metrics}
	The direction chosen for evaluation is how well the clusters found separate the initial dataset by beer types. In order to do this, each cluster is labeled with the most common style of beer among its elements. This turns our problem intro a multi-label classification one.
	
	The metrics used are the ones described by Beleites at al.\cite{MultilabelClassification} for multi-class problems. First of all, a confusion matrix is computed, which is a $l\cdot l$ matrix where l is the number of features. Each element (x, y) in the confusion matrix has a value meaning how many times a data point with label x has been classified as having label y. Precision and recall are then calculated for each label, simulating the binary evaluation. 
	
	Precision, also called positive predictive value, is a fraction that represents how many elements classified with a label have been assigned correctly. Recall, also known as sensitivity, is the fraction of instances correctly assigned to a class and all instances from that class. For the binary case, precision and recall can be calculated as in figure \ref{eq:PrecRecall}, where tp, fp, fn stand for true positives, false positives and false negatives. This can be extended to the confusion matrix, for each row, as follows: precision is the fraction of the value on the diagonal and the sum of values on the column, recall is the value on the diagonal divided by the sum of values on the row. The precision and recall of the multi-label classifier can then be computed as the means of the values obtained from the previous step.
	
	\begin{equation}
	\label{eq:PrecRecall}
	precision = \frac{tp}{tp + fp} \quad recall = \frac{tp}{tp + fn}
	\end{equation}
	
	F1 score is a combined measure of precision and recall, used to provide a single measurement for the system. It represents the harmonic mean of the two values, as in figure \ref{eq:F1}, where mp and mr are mean precision and mean recall.
	
	\begin{equation}
	\label{eq:F1}
	F1 = \frac{2 * mp * mr}{mp + mr}
	\end{equation}
	
	\subsection{Results}
	\subsubsection{Multi-label classification score}
	The correlation between the beer styles of the data points and the type that is most prevalent in the resulting clusters was chose as a measure of evaluation. For the best run, the result can be seen in table \ref{tab:results}. These were obtained using the random seed initialization method.
	
	\begin{center}
		\label{tab:results}
		\begin{tabular}{ |c|c| } 
			\hline
			Mean precision & 0.537 \\
			Mean recall & 0.532 \\
			F1 score & 0.534 \\
			\hline
		\end{tabular}
	\end{center}

	\subsubsection{Heatmap}
	In order to better visualize the results a heatmap was generated. Visible in figure \ref{fig:heatmap}, the diagram helps visualize which style of beer is more common in which cluster. As expected, there is a clear separation for most of the dominant styles in each cluster. This is best observed for the following cluster, style pairs: 0 with Porter, 1 with Stout, 2 with Pale Lager, 4 with Wheat Beer, 5 with Belgian Ale, 6 with Fruit/Vegetable Beer and 7 with Barleywine. For cluster 3 there are two dominant classes : Pale Ale and Amber, which can be explained by the fact that Amber is a particular style of ale. For cluster 8 there is no clear majority, meaning it might be a collection of outliers from other styles.
	
	\begin{figure}
		\includegraphics[width=\linewidth]{resources/9500rheatmap.png}
		\caption{Heatmap of beer styles per cluster}
		\label{fig:heatmap}
	\end{figure}
	
	\subsubsection{Common words per cluster}
	In order to interpret what features these clusters have in common, we can look at the most popular words for each cluster, identified by id. Figure \ref{fig:words} shows the 10 words with the highest tf-idf score for the mean of each cluster. Looking at this figure we can find the reason for the shape of cluster 8, as the most common words are general words used in beer description, like 'beer', 'taste', 'drink', 'flavour', which can be applied to any beer style. 
	
	Another thing we can observe from the table of common words is the difference between 'Porter' and 'Stout' styles, which are often put under the same category. According to the diagram, Porter beers tend to have a stronger chocolate, roast, coffee taste while Stouts also have a fruity, citrus taste.
	
	An interesting observation is that while the Wheat Beer style is clearly associated with cluster number 4, the feature words for this cluster: 'chocolate', 'dark', 'roast' are not at all representative of the style.
	
	\begin{figure}
		\includegraphics[width=\linewidth]{resources/words.png}
		\caption{Most common words for each cluster}
		\label{fig:words}
	\end{figure}
	
	\section{Conclusions}
	The results of the clustering algorithm were, as expected, clusters around similar beer styles. In terms of multi-label classification the algorithm obtained a F1 score of 0.53 on 9 classes, which indicates there is a correlation. This was also confirmed by the heatmap of the clusters which indicated the poor performance on some of the beer styles. The means of the clusters tend to contain words descriptive of their associated styles with some exceptions.
	
	In terms of improvement, the paper shows that a more sophisticated seed selection method does not offer better results. Another direction worth researching is using more data. This experiment used only 85500 reviews out of the total 1.5 million, which could also be augment with data from other beer review websites. For this to be feasible some computation time optimizations would be useful. 
	
	\newpage
	\bibliography{references_document}
	\bibliographystyle{ieeetr}
\end{document}